{"file_name": "/home/qj213/afp-2021-10-22/thys/Randomised_Social_Choice/Random_Dictatorship.thy", "working_directory": "/home/qj213/afp-2021-10-22/thys/Randomised_Social_Choice", "problem_names": ["lemma random_dictatorship_def:\n  assumes \"is_pref_profile R\"\n  shows  \"RD R =\n            do {\n              i \\<leftarrow> pmf_of_set agents; \n              pmf_of_set (favorites R i)\n            }\"", "lemma random_dictatorship_unique_favorites:\n  assumes \"is_pref_profile R\" \"has_unique_favorites R\"\n  shows   \"RD R = map_pmf (favorite R) (pmf_of_set agents)\"", "lemma random_dictatorship_unique_favorites':\n  assumes \"is_pref_profile R\" \"has_unique_favorites R\"\n  shows   \"RD R = pmf_of_multiset (image_mset (favorite R) (mset_set agents))\"", "lemma pmf_random_dictatorship:\n  assumes \"is_pref_profile R\"\n  shows \"pmf (RD R) x =\n           (\\<Sum>i\\<in>agents. indicator (favorites R i) x /\n              real (card (favorites R i))) / real (card agents)\""], "translations": [["", "lemma random_dictatorship_def:\n  assumes \"is_pref_profile R\"\n  shows  \"RD R =\n            do {\n              i \\<leftarrow> pmf_of_set agents; \n              pmf_of_set (favorites R i)\n            }\""], ["proof (prove)\ngoal (1 subgoal):\n 1. RD R =\n    pmf_of_set agents \\<bind> (\\<lambda>i. pmf_of_set (favorites R i))", "proof -"], ["proof (state)\ngoal (1 subgoal):\n 1. RD R =\n    pmf_of_set agents \\<bind> (\\<lambda>i. pmf_of_set (favorites R i))", "from assms"], ["proof (chain)\npicking this:\n  is_pref_profile R", "interpret pref_profile_wf agents alts R"], ["proof (prove)\nusing this:\n  is_pref_profile R\n\ngoal (1 subgoal):\n 1. is_pref_profile R", "."], ["proof (state)\ngoal (1 subgoal):\n 1. RD R =\n    pmf_of_set agents \\<bind> (\\<lambda>i. pmf_of_set (favorites R i))", "show ?thesis"], ["proof (prove)\ngoal (1 subgoal):\n 1. RD R =\n    pmf_of_set agents \\<bind> (\\<lambda>i. pmf_of_set (favorites R i))", "by (simp add: random_dictatorship_auxdef favorites_altdef)"], ["proof (state)\nthis:\n  RD R = pmf_of_set agents \\<bind> (\\<lambda>i. pmf_of_set (favorites R i))\n\ngoal:\nNo subgoals!", "qed"], ["", "lemma random_dictatorship_unique_favorites:\n  assumes \"is_pref_profile R\" \"has_unique_favorites R\"\n  shows   \"RD R = map_pmf (favorite R) (pmf_of_set agents)\""], ["proof (prove)\ngoal (1 subgoal):\n 1. RD R = map_pmf (favorite R) (pmf_of_set agents)", "proof -"], ["proof (state)\ngoal (1 subgoal):\n 1. RD R = map_pmf (favorite R) (pmf_of_set agents)", "from assms(1)"], ["proof (chain)\npicking this:\n  is_pref_profile R", "interpret pref_profile_wf agents alts R"], ["proof (prove)\nusing this:\n  is_pref_profile R\n\ngoal (1 subgoal):\n 1. is_pref_profile R", "."], ["proof (state)\ngoal (1 subgoal):\n 1. RD R = map_pmf (favorite R) (pmf_of_set agents)", "from assms(2)"], ["proof (chain)\npicking this:\n  has_unique_favorites R", "interpret pref_profile_unique_favorites agents alts R"], ["proof (prove)\nusing this:\n  has_unique_favorites R\n\ngoal (1 subgoal):\n 1. pref_profile_unique_favorites agents alts R", "by unfold_locales"], ["proof (state)\ngoal (1 subgoal):\n 1. RD R = map_pmf (favorite R) (pmf_of_set agents)", "show ?thesis"], ["proof (prove)\ngoal (1 subgoal):\n 1. RD R = map_pmf (favorite R) (pmf_of_set agents)", "unfolding random_dictatorship_def[OF assms(1)] map_pmf_def"], ["proof (prove)\ngoal (1 subgoal):\n 1. pmf_of_set agents \\<bind> (\\<lambda>i. pmf_of_set (favorites R i)) =\n    pmf_of_set agents \\<bind> (\\<lambda>x. return_pmf (favorite R x))", "by (intro bind_pmf_cong) (auto simp: unique_favorites pmf_of_set_singleton)"], ["proof (state)\nthis:\n  RD R = map_pmf (favorite R) (pmf_of_set agents)\n\ngoal:\nNo subgoals!", "qed"], ["", "lemma random_dictatorship_unique_favorites':\n  assumes \"is_pref_profile R\" \"has_unique_favorites R\"\n  shows   \"RD R = pmf_of_multiset (image_mset (favorite R) (mset_set agents))\""], ["proof (prove)\ngoal (1 subgoal):\n 1. RD R = pmf_of_multiset (image_mset (favorite R) (mset_set agents))", "using assms"], ["proof (prove)\nusing this:\n  is_pref_profile R\n  has_unique_favorites R\n\ngoal (1 subgoal):\n 1. RD R = pmf_of_multiset (image_mset (favorite R) (mset_set agents))", "by (simp add: random_dictatorship_unique_favorites map_pmf_of_set)"], ["", "lemma pmf_random_dictatorship:\n  assumes \"is_pref_profile R\"\n  shows \"pmf (RD R) x =\n           (\\<Sum>i\\<in>agents. indicator (favorites R i) x /\n              real (card (favorites R i))) / real (card agents)\""], ["proof (prove)\ngoal (1 subgoal):\n 1. pmf (RD R) x =\n    (\\<Sum>i\\<in>agents.\n       indicat_real (favorites R i) x / real (card (favorites R i))) /\n    real (card agents)", "proof -"], ["proof (state)\ngoal (1 subgoal):\n 1. pmf (RD R) x =\n    (\\<Sum>i\\<in>agents.\n       indicat_real (favorites R i) x / real (card (favorites R i))) /\n    real (card agents)", "from assms(1)"], ["proof (chain)\npicking this:\n  is_pref_profile R", "interpret pref_profile_wf agents alts R"], ["proof (prove)\nusing this:\n  is_pref_profile R\n\ngoal (1 subgoal):\n 1. is_pref_profile R", "."], ["proof (state)\ngoal (1 subgoal):\n 1. pmf (RD R) x =\n    (\\<Sum>i\\<in>agents.\n       indicat_real (favorites R i) x / real (card (favorites R i))) /\n    real (card agents)", "from nonempty_dom"], ["proof (chain)\npicking this:\n  agents \\<noteq> {}", "have \"card agents > 0\""], ["proof (prove)\nusing this:\n  agents \\<noteq> {}\n\ngoal (1 subgoal):\n 1. 0 < card agents", "by (auto simp del: nonempty_agents)"], ["proof (state)\nthis:\n  0 < card agents\n\ngoal (1 subgoal):\n 1. pmf (RD R) x =\n    (\\<Sum>i\\<in>agents.\n       indicat_real (favorites R i) x / real (card (favorites R i))) /\n    real (card agents)", "hence \"ennreal (pmf (RD R) x) = \n           ennreal ((\\<Sum>i\\<in>agents. pmf (pmf_of_set (favorites R i)) x) / real (card agents))\"\n    (is \"_ = ennreal (?p / _)\")"], ["proof (prove)\nusing this:\n  0 < card agents\n\ngoal (1 subgoal):\n 1. ennreal (pmf (RD R) x) =\n    ennreal\n     ((\\<Sum>i\\<in>agents. pmf (pmf_of_set (favorites R i)) x) /\n      real (card agents))", "unfolding random_dictatorship_def[OF assms]"], ["proof (prove)\nusing this:\n  0 < card agents\n\ngoal (1 subgoal):\n 1. ennreal\n     (pmf (pmf_of_set agents \\<bind>\n           (\\<lambda>i. pmf_of_set (favorites R i)))\n       x) =\n    ennreal\n     ((\\<Sum>i\\<in>agents. pmf (pmf_of_set (favorites R i)) x) /\n      real (card agents))", "by (simp_all add: ennreal_pmf_bind nn_integral_pmf_of_set max_def \n          divide_ennreal [symmetric] ennreal_of_nat_eq_real_of_nat sum_nonneg)"], ["proof (state)\nthis:\n  ennreal (pmf (RD R) x) =\n  ennreal\n   ((\\<Sum>i\\<in>agents. pmf (pmf_of_set (favorites R i)) x) /\n    real (card agents))\n\ngoal (1 subgoal):\n 1. pmf (RD R) x =\n    (\\<Sum>i\\<in>agents.\n       indicat_real (favorites R i) x / real (card (favorites R i))) /\n    real (card agents)", "also"], ["proof (state)\nthis:\n  ennreal (pmf (RD R) x) =\n  ennreal\n   ((\\<Sum>i\\<in>agents. pmf (pmf_of_set (favorites R i)) x) /\n    real (card agents))\n\ngoal (1 subgoal):\n 1. pmf (RD R) x =\n    (\\<Sum>i\\<in>agents.\n       indicat_real (favorites R i) x / real (card (favorites R i))) /\n    real (card agents)", "have \"?p = (\\<Sum>i\\<in>agents. indicator (favorites R i) x / real (card (favorites R i)))\""], ["proof (prove)\ngoal (1 subgoal):\n 1. (\\<Sum>i\\<in>agents. pmf (pmf_of_set (favorites R i)) x) =\n    (\\<Sum>i\\<in>agents.\n       indicat_real (favorites R i) x / real (card (favorites R i)))", "by (intro sum.cong) (simp_all add: favorites_nonempty)"], ["proof (state)\nthis:\n  (\\<Sum>i\\<in>agents. pmf (pmf_of_set (favorites R i)) x) =\n  (\\<Sum>i\\<in>agents.\n     indicat_real (favorites R i) x / real (card (favorites R i)))\n\ngoal (1 subgoal):\n 1. pmf (RD R) x =\n    (\\<Sum>i\\<in>agents.\n       indicat_real (favorites R i) x / real (card (favorites R i))) /\n    real (card agents)", "finally"], ["proof (chain)\npicking this:\n  ennreal (pmf (RD R) x) =\n  ennreal\n   ((\\<Sum>i\\<in>agents.\n       indicat_real (favorites R i) x / real (card (favorites R i))) /\n    real (card agents))", "show ?thesis"], ["proof (prove)\nusing this:\n  ennreal (pmf (RD R) x) =\n  ennreal\n   ((\\<Sum>i\\<in>agents.\n       indicat_real (favorites R i) x / real (card (favorites R i))) /\n    real (card agents))\n\ngoal (1 subgoal):\n 1. pmf (RD R) x =\n    (\\<Sum>i\\<in>agents.\n       indicat_real (favorites R i) x / real (card (favorites R i))) /\n    real (card agents)", "by (subst (asm) ennreal_inj) (auto intro!: sum_nonneg divide_nonneg_nonneg)"], ["proof (state)\nthis:\n  pmf (RD R) x =\n  (\\<Sum>i\\<in>agents.\n     indicat_real (favorites R i) x / real (card (favorites R i))) /\n  real (card agents)\n\ngoal:\nNo subgoals!", "qed"], ["", "sublocale RD: social_decision_scheme agents alts RD"], ["proof (prove)\ngoal (1 subgoal):\n 1. social_decision_scheme agents alts RD", "proof"], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>R. is_pref_profile R \\<Longrightarrow> RD R \\<in> lotteries", "fix R"], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>R. is_pref_profile R \\<Longrightarrow> RD R \\<in> lotteries", "assume R_wf: \"is_pref_profile R\""], ["proof (state)\nthis:\n  is_pref_profile R\n\ngoal (1 subgoal):\n 1. \\<And>R. is_pref_profile R \\<Longrightarrow> RD R \\<in> lotteries", "then"], ["proof (chain)\npicking this:\n  is_pref_profile R", "interpret pref_profile_wf agents alts R"], ["proof (prove)\nusing this:\n  is_pref_profile R\n\ngoal (1 subgoal):\n 1. is_pref_profile R", "."], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>R. is_pref_profile R \\<Longrightarrow> RD R \\<in> lotteries", "from R_wf"], ["proof (chain)\npicking this:\n  is_pref_profile R", "show \"RD R \\<in> lotteries\""], ["proof (prove)\nusing this:\n  is_pref_profile R\n\ngoal (1 subgoal):\n 1. RD R \\<in> lotteries", "using favorites_subset_alts favorites_nonempty"], ["proof (prove)\nusing this:\n  is_pref_profile R\n  favorites R ?i \\<subseteq> alts\n  ?i \\<in> agents \\<Longrightarrow> favorites R ?i \\<noteq> {}\n\ngoal (1 subgoal):\n 1. RD R \\<in> lotteries", "by (auto simp: lotteries_on_def random_dictatorship_def)"], ["proof (state)\nthis:\n  RD R \\<in> lotteries\n\ngoal:\nNo subgoals!", "qed"], ["", "text \\<open>\n  We now show that Random Dictatorship fulfils anonymity, neutrality, \n  and strong strategyproofness.\n  At the very least, this shows that the definitions of these notions are \n  consistent.\n\\<close>"], ["", "subsection \\<open>Anonymity\\<close>"], ["", "text \\<open>\n  The following proof is essentially the following:\n  In Random Dictatorship, permuting the agents in the preference profile is the same\n  as applying the permutation to the agent that was picked uniformly at random in the \n  first step. However, uniform distributions are invariant under permutation, therefore\n  the outcome is totally unchanged.\n\\<close>"], ["", "sublocale RD: anonymous_sds agents alts RD"], ["proof (prove)\ngoal (1 subgoal):\n 1. anonymous_sds agents alts RD", "proof"], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>\\<pi> R.\n       \\<lbrakk>\\<pi> permutes agents; is_pref_profile R\\<rbrakk>\n       \\<Longrightarrow> RD (R \\<circ> \\<pi>) = RD R", "fix R \\<pi>"], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>\\<pi> R.\n       \\<lbrakk>\\<pi> permutes agents; is_pref_profile R\\<rbrakk>\n       \\<Longrightarrow> RD (R \\<circ> \\<pi>) = RD R", "assume wf: \"is_pref_profile R\" and perm: \"\\<pi> permutes agents\""], ["proof (state)\nthis:\n  is_pref_profile R\n  \\<pi> permutes agents\n\ngoal (1 subgoal):\n 1. \\<And>\\<pi> R.\n       \\<lbrakk>\\<pi> permutes agents; is_pref_profile R\\<rbrakk>\n       \\<Longrightarrow> RD (R \\<circ> \\<pi>) = RD R", "interpret pref_profile_wf agents alts R"], ["proof (prove)\ngoal (1 subgoal):\n 1. is_pref_profile R", "by fact"], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>\\<pi> R.\n       \\<lbrakk>\\<pi> permutes agents; is_pref_profile R\\<rbrakk>\n       \\<Longrightarrow> RD (R \\<circ> \\<pi>) = RD R", "from wf_permute_agents[OF perm]"], ["proof (chain)\npicking this:\n  is_pref_profile (R \\<circ> \\<pi>)", "have \"RD (R \\<circ> \\<pi>) = map_pmf \\<pi> (pmf_of_set agents) \\<bind> (\\<lambda>i. pmf_of_set (favorites R i))\""], ["proof (prove)\nusing this:\n  is_pref_profile (R \\<circ> \\<pi>)\n\ngoal (1 subgoal):\n 1. RD (R \\<circ> \\<pi>) =\n    map_pmf \\<pi> (pmf_of_set agents) \\<bind>\n    (\\<lambda>i. pmf_of_set (favorites R i))", "by (simp add: bind_map_pmf random_dictatorship_def o_def favorites_def)"], ["proof (state)\nthis:\n  RD (R \\<circ> \\<pi>) =\n  map_pmf \\<pi> (pmf_of_set agents) \\<bind>\n  (\\<lambda>i. pmf_of_set (favorites R i))\n\ngoal (1 subgoal):\n 1. \\<And>\\<pi> R.\n       \\<lbrakk>\\<pi> permutes agents; is_pref_profile R\\<rbrakk>\n       \\<Longrightarrow> RD (R \\<circ> \\<pi>) = RD R", "also"], ["proof (state)\nthis:\n  RD (R \\<circ> \\<pi>) =\n  map_pmf \\<pi> (pmf_of_set agents) \\<bind>\n  (\\<lambda>i. pmf_of_set (favorites R i))\n\ngoal (1 subgoal):\n 1. \\<And>\\<pi> R.\n       \\<lbrakk>\\<pi> permutes agents; is_pref_profile R\\<rbrakk>\n       \\<Longrightarrow> RD (R \\<circ> \\<pi>) = RD R", "from perm wf"], ["proof (chain)\npicking this:\n  \\<pi> permutes agents\n  is_pref_profile R", "have \"\\<dots> = RD R\""], ["proof (prove)\nusing this:\n  \\<pi> permutes agents\n  is_pref_profile R\n\ngoal (1 subgoal):\n 1. map_pmf \\<pi> (pmf_of_set agents) \\<bind>\n    (\\<lambda>i. pmf_of_set (favorites R i)) =\n    RD R", "by (simp add: map_pmf_of_set_inj permutes_inj_on permutes_image random_dictatorship_def)"], ["proof (state)\nthis:\n  map_pmf \\<pi> (pmf_of_set agents) \\<bind>\n  (\\<lambda>i. pmf_of_set (favorites R i)) =\n  RD R\n\ngoal (1 subgoal):\n 1. \\<And>\\<pi> R.\n       \\<lbrakk>\\<pi> permutes agents; is_pref_profile R\\<rbrakk>\n       \\<Longrightarrow> RD (R \\<circ> \\<pi>) = RD R", "finally"], ["proof (chain)\npicking this:\n  RD (R \\<circ> \\<pi>) = RD R", "show \"RD (R \\<circ> \\<pi>) = RD R\""], ["proof (prove)\nusing this:\n  RD (R \\<circ> \\<pi>) = RD R\n\ngoal (1 subgoal):\n 1. RD (R \\<circ> \\<pi>) = RD R", "."], ["proof (state)\nthis:\n  RD (R \\<circ> \\<pi>) = RD R\n\ngoal:\nNo subgoals!", "qed"], ["", "subsection \\<open>Neutrality\\<close>"], ["", "text \\<open>\n  The proof of neutrality is similar to that of anonymity. We have proven elsewhere\n  that the most preferred alternatives of an agent in a profile with permuted alternatives\n  are simply the image of the originally preferred alternatives.\n  Since we pick one alternative from the most preferred alternatives of the selected agent\n  uniformly at random, this means that we effectively pick an agent, then pick on of her \n  most preferred alternatives, and then apply the permutation to that alternative, \n  which is simply Random Dictatorship transformed with the permutation.\n\\<close>"], ["", "sublocale RD: neutral_sds agents alts RD"], ["proof (prove)\ngoal (1 subgoal):\n 1. neutral_sds agents alts RD", "proof"], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>\\<sigma> R.\n       \\<lbrakk>\\<sigma> permutes alts; is_pref_profile R\\<rbrakk>\n       \\<Longrightarrow> RD (permute_profile \\<sigma> R) =\n                         map_pmf \\<sigma> (RD R)", "fix \\<sigma> R"], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>\\<sigma> R.\n       \\<lbrakk>\\<sigma> permutes alts; is_pref_profile R\\<rbrakk>\n       \\<Longrightarrow> RD (permute_profile \\<sigma> R) =\n                         map_pmf \\<sigma> (RD R)", "assume perm: \"\\<sigma> permutes alts\" and R_wf: \"is_pref_profile R\""], ["proof (state)\nthis:\n  \\<sigma> permutes alts\n  is_pref_profile R\n\ngoal (1 subgoal):\n 1. \\<And>\\<sigma> R.\n       \\<lbrakk>\\<sigma> permutes alts; is_pref_profile R\\<rbrakk>\n       \\<Longrightarrow> RD (permute_profile \\<sigma> R) =\n                         map_pmf \\<sigma> (RD R)", "from R_wf"], ["proof (chain)\npicking this:\n  is_pref_profile R", "interpret pref_profile_wf agents alts R"], ["proof (prove)\nusing this:\n  is_pref_profile R\n\ngoal (1 subgoal):\n 1. is_pref_profile R", "."], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>\\<sigma> R.\n       \\<lbrakk>\\<sigma> permutes alts; is_pref_profile R\\<rbrakk>\n       \\<Longrightarrow> RD (permute_profile \\<sigma> R) =\n                         map_pmf \\<sigma> (RD R)", "from wf_permute_alts[OF perm] R_wf perm"], ["proof (chain)\npicking this:\n  is_pref_profile (permute_profile \\<sigma> R)\n  is_pref_profile R\n  \\<sigma> permutes alts", "show \"RD (permute_profile \\<sigma> R) = map_pmf \\<sigma> (RD R)\""], ["proof (prove)\nusing this:\n  is_pref_profile (permute_profile \\<sigma> R)\n  is_pref_profile R\n  \\<sigma> permutes alts\n\ngoal (1 subgoal):\n 1. RD (permute_profile \\<sigma> R) = map_pmf \\<sigma> (RD R)", "by (subst random_dictatorship_def)\n       (auto intro!: bind_pmf_cong simp: random_dictatorship_def map_bind_pmf \n          favorites_permute map_pmf_of_set_inj permutes_inj_on favorites_nonempty)"], ["proof (state)\nthis:\n  RD (permute_profile \\<sigma> R) = map_pmf \\<sigma> (RD R)\n\ngoal:\nNo subgoals!", "qed"], ["", "subsection \\<open>Strong strategyproofness\\<close>"], ["", "text \\<open>\n  The argument for strategyproofness is quite simple:\n  Since the preferences submitted by an agent @{term i} only influence \n  the outcome when that agent is picked in the first process, it suffices \n  to focus on this case.\n  When the agent @{term i} submits her true preferences, the probability of \n  obtaining a result at least as good as @{term x} (for any alternative @{term x})\n  is 1, since the outcome will always be one of her most-preferred alternatives.\n  Obviously, the probability of obtaining such a result cannot exceed 1 no matter\n  what preferences she submits instead, and thus, RD is strategyproof.\n\\<close>"], ["", "sublocale RD: strongly_strategyproof_sds agents alts RD"], ["proof (prove)\ngoal (1 subgoal):\n 1. strongly_strategyproof_sds agents alts RD", "proof (unfold_locales, unfold RD.strongly_strategyproof_profile_def)"], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>R i Ri'.\n       \\<lbrakk>is_pref_profile R; i \\<in> agents;\n        total_preorder_on alts Ri'\\<rbrakk>\n       \\<Longrightarrow> SD (R i) (RD (R(i := Ri'))) (RD R)", "fix R i Ri'"], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>R i Ri'.\n       \\<lbrakk>is_pref_profile R; i \\<in> agents;\n        total_preorder_on alts Ri'\\<rbrakk>\n       \\<Longrightarrow> SD (R i) (RD (R(i := Ri'))) (RD R)", "assume R_wf: \"is_pref_profile R\" and i: \"i \\<in> agents\"\n                 and Ri'_wf: \"total_preorder_on alts Ri'\""], ["proof (state)\nthis:\n  is_pref_profile R\n  i \\<in> agents\n  total_preorder_on alts Ri'\n\ngoal (1 subgoal):\n 1. \\<And>R i Ri'.\n       \\<lbrakk>is_pref_profile R; i \\<in> agents;\n        total_preorder_on alts Ri'\\<rbrakk>\n       \\<Longrightarrow> SD (R i) (RD (R(i := Ri'))) (RD R)", "interpret R: pref_profile_wf agents alts R"], ["proof (prove)\ngoal (1 subgoal):\n 1. is_pref_profile R", "by fact"], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>R i Ri'.\n       \\<lbrakk>is_pref_profile R; i \\<in> agents;\n        total_preorder_on alts Ri'\\<rbrakk>\n       \\<Longrightarrow> SD (R i) (RD (R(i := Ri'))) (RD R)", "from R_wf Ri'_wf i"], ["proof (chain)\npicking this:\n  is_pref_profile R\n  total_preorder_on alts Ri'\n  i \\<in> agents", "have R'_wf: \"is_pref_profile (R(i := Ri'))\""], ["proof (prove)\nusing this:\n  is_pref_profile R\n  total_preorder_on alts Ri'\n  i \\<in> agents\n\ngoal (1 subgoal):\n 1. is_pref_profile (R(i := Ri'))", "by (simp add: R.wf_update)"], ["proof (state)\nthis:\n  is_pref_profile (R(i := Ri'))\n\ngoal (1 subgoal):\n 1. \\<And>R i Ri'.\n       \\<lbrakk>is_pref_profile R; i \\<in> agents;\n        total_preorder_on alts Ri'\\<rbrakk>\n       \\<Longrightarrow> SD (R i) (RD (R(i := Ri'))) (RD R)", "interpret R': pref_profile_wf agents alts \"R(i := Ri')\""], ["proof (prove)\ngoal (1 subgoal):\n 1. is_pref_profile (R(i := Ri'))", "by fact"], ["proof (state)\ngoal (1 subgoal):\n 1. \\<And>R i Ri'.\n       \\<lbrakk>is_pref_profile R; i \\<in> agents;\n        total_preorder_on alts Ri'\\<rbrakk>\n       \\<Longrightarrow> SD (R i) (RD (R(i := Ri'))) (RD R)", "show \"SD (R i) (RD (R(i := Ri'))) (RD R)\""], ["proof (prove)\ngoal (1 subgoal):\n 1. SD (R i) (RD (R(i := Ri'))) (RD R)", "proof (rule R.SD_pref_profileI)"], ["proof (state)\ngoal (4 subgoals):\n 1. i \\<in> agents\n 2. RD R \\<in> lotteries\n 3. RD (R(i := Ri')) \\<in> lotteries\n 4. \\<And>x.\n       x \\<in> alts \\<Longrightarrow>\n       lottery_prob (RD (R(i := Ri'))) (preferred_alts (R i) x)\n       \\<le> lottery_prob (RD R) (preferred_alts (R i) x)", "fix x"], ["proof (state)\ngoal (4 subgoals):\n 1. i \\<in> agents\n 2. RD R \\<in> lotteries\n 3. RD (R(i := Ri')) \\<in> lotteries\n 4. \\<And>x.\n       x \\<in> alts \\<Longrightarrow>\n       lottery_prob (RD (R(i := Ri'))) (preferred_alts (R i) x)\n       \\<le> lottery_prob (RD R) (preferred_alts (R i) x)", "assume \"x \\<in> alts\""], ["proof (state)\nthis:\n  x \\<in> alts\n\ngoal (4 subgoals):\n 1. i \\<in> agents\n 2. RD R \\<in> lotteries\n 3. RD (R(i := Ri')) \\<in> lotteries\n 4. \\<And>x.\n       x \\<in> alts \\<Longrightarrow>\n       lottery_prob (RD (R(i := Ri'))) (preferred_alts (R i) x)\n       \\<le> lottery_prob (RD R) (preferred_alts (R i) x)", "hence \"emeasure (measure_pmf (RD (R(i := Ri')))) (preferred_alts (R i) x)\n             \\<le> emeasure (measure_pmf (RD R)) (preferred_alts (R i) x)\""], ["proof (prove)\nusing this:\n  x \\<in> alts\n\ngoal (1 subgoal):\n 1. emeasure (measure_pmf (RD (R(i := Ri')))) (preferred_alts (R i) x)\n    \\<le> emeasure (measure_pmf (RD R)) (preferred_alts (R i) x)", "using Ri'_wf maximal_imp_preferred[of \"R i\" x]"], ["proof (prove)\nusing this:\n  x \\<in> alts\n  total_preorder_on alts Ri'\n  \\<lbrakk>total_preorder_on alts (R i); x \\<in> alts\\<rbrakk>\n  \\<Longrightarrow> Max_wrt (R i) \\<subseteq> preferred_alts (R i) x\n\ngoal (1 subgoal):\n 1. emeasure (measure_pmf (RD (R(i := Ri')))) (preferred_alts (R i) x)\n    \\<le> emeasure (measure_pmf (RD R)) (preferred_alts (R i) x)", "by (auto intro!: card_mono nn_integral_mono_AE \n               simp: random_dictatorship_def R_wf R'_wf AE_measure_pmf_iff Max_wrt_prefs_finite\n                     emeasure_pmf_of_set Int_absorb2 favorites_def\n                     Max_wrt_prefs_nonempty card_gt_0_iff)"], ["proof (state)\nthis:\n  emeasure (measure_pmf (RD (R(i := Ri')))) (preferred_alts (R i) x)\n  \\<le> emeasure (measure_pmf (RD R)) (preferred_alts (R i) x)\n\ngoal (4 subgoals):\n 1. i \\<in> agents\n 2. RD R \\<in> lotteries\n 3. RD (R(i := Ri')) \\<in> lotteries\n 4. \\<And>x.\n       x \\<in> alts \\<Longrightarrow>\n       lottery_prob (RD (R(i := Ri'))) (preferred_alts (R i) x)\n       \\<le> lottery_prob (RD R) (preferred_alts (R i) x)", "thus \"lottery_prob (RD (R(i := Ri'))) (preferred_alts (R i) x)\n            \\<le> lottery_prob (RD R) (preferred_alts (R i) x)\""], ["proof (prove)\nusing this:\n  emeasure (measure_pmf (RD (R(i := Ri')))) (preferred_alts (R i) x)\n  \\<le> emeasure (measure_pmf (RD R)) (preferred_alts (R i) x)\n\ngoal (1 subgoal):\n 1. lottery_prob (RD (R(i := Ri'))) (preferred_alts (R i) x)\n    \\<le> lottery_prob (RD R) (preferred_alts (R i) x)", "by (simp add: measure_pmf.emeasure_eq_measure)"], ["proof (state)\nthis:\n  lottery_prob (RD (R(i := Ri'))) (preferred_alts (R i) x)\n  \\<le> lottery_prob (RD R) (preferred_alts (R i) x)\n\ngoal (3 subgoals):\n 1. i \\<in> agents\n 2. RD R \\<in> lotteries\n 3. RD (R(i := Ri')) \\<in> lotteries", "qed (insert R_wf R'_wf, simp_all add: RD.sds_wf i)"], ["proof (state)\nthis:\n  SD (R i) (RD (R(i := Ri'))) (RD R)\n\ngoal:\nNo subgoals!", "qed"], ["", "end"], ["", "end"]]}